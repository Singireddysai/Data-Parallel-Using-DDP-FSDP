{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:15:08.502955Z","iopub.execute_input":"2026-01-05T07:15:08.503547Z","iopub.status.idle":"2026-01-05T07:15:08.802272Z","shell.execute_reply.started":"2026-01-05T07:15:08.503516Z","shell.execute_reply":"2026-01-05T07:15:08.801678Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# DDP with 2 GPUs","metadata":{}},{"cell_type":"code","source":"%%writefile train_ddp.py\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed import init_process_group, destroy_process_group\nimport torch.distributed as dist\n\n\ndef ddp_setup():\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    dist.init_process_group(\n        backend=\"nccl\",\n        device_id=local_rank\n    )\n    torch.cuda.set_device(local_rank)\n    return local_rank\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n        self.conv3 = nn.Conv2d(64, 128, 5, padding=2)\n        self.pool = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(128 * 3 * 3, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = self.pool(self.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)\n\ndef main():\n    local_rank = ddp_setup()\n    rank = torch.distributed.get_rank()\n    world_size = torch.distributed.get_world_size()\n\n    if rank == 0:\n        print(f\"Running DDP on {world_size} GPUs\")\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    # Download only on rank 0\n    if rank == 0:\n        datasets.MNIST(\"./data\", train=True, download=True)\n        datasets.MNIST(\"./data\", train=False, download=True)\n    torch.distributed.barrier()\n\n    train_dataset = datasets.MNIST(\"./data\", train=True, transform=transform)\n    train_sampler = DistributedSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=128,\n        sampler=train_sampler,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n    test_loader = DataLoader(test_dataset, batch_size=128)\n\n    model = CNN().cuda()\n    model = DDP(model, device_ids=[local_rank])\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(5):\n        model.train()\n        train_sampler.set_epoch(epoch)\n\n        correct = total = 0\n        for x, y in train_loader:\n            x = x.cuda(non_blocking=True)\n            y = y.cuda(non_blocking=True)\n\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n\n            correct += (out.argmax(1) == y).sum().item()\n            total += y.size(0)\n\n        if rank == 0:\n            print(f\"Epoch {epoch+1} | Train Acc: {100*correct/total:.2f}%\")\n\n    if rank == 0:\n        model.eval()\n        correct = total = 0\n        with torch.no_grad():\n            for x, y in test_loader:\n                x, y = x.cuda(), y.cuda()\n                out = model.module(x)\n                correct += (out.argmax(1) == y).sum().item()\n                total += y.size(0)\n        print(f\"\\nTest Accuracy: {100*correct/total:.2f}%\")\n\n    destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:15:08.803433Z","iopub.execute_input":"2026-01-05T07:15:08.803796Z","iopub.status.idle":"2026-01-05T07:15:08.811346Z","shell.execute_reply.started":"2026-01-05T07:15:08.803772Z","shell.execute_reply":"2026-01-05T07:15:08.810536Z"}},"outputs":[{"name":"stdout","text":"Writing train_ddp.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Check how many GPUs you have\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:15:08.812313Z","iopub.execute_input":"2026-01-05T07:15:08.812589Z","iopub.status.idle":"2026-01-05T07:15:12.699276Z","shell.execute_reply.started":"2026-01-05T07:15:08.812556Z","shell.execute_reply":"2026-01-05T07:15:12.698473Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 2\nGPU Name: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n\n!torchrun --standalone --nproc_per_node=2 train_ddp.py   # Use 1 for single GPU (safe)\n# OR if you have 2 GPUs (Pro/Pro+):\n# !torchrun --standalone --nproc_per_node=2 train_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:15:12.700893Z","iopub.execute_input":"2026-01-05T07:15:12.701438Z","iopub.status.idle":"2026-01-05T07:16:05.082365Z","shell.execute_reply.started":"2026-01-05T07:15:12.701413Z","shell.execute_reply":"2026-01-05T07:16:05.081495Z"}},"outputs":[{"name":"stdout","text":"W0105 07:15:14.682000 119 torch/distributed/run.py:774] \nW0105 07:15:14.682000 119 torch/distributed/run.py:774] *****************************************\nW0105 07:15:14.682000 119 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0105 07:15:14.682000 119 torch/distributed/run.py:774] *****************************************\n[W105 07:15:14.103082572 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:15:14.103945206 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:15:20.940558886 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:15:20.940645380 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:15:20.941322093 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:15:20.941922054 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nRunning DDP on 2 GPUs\n100%|██████████████████████████████████████| 9.91M/9.91M [00:00<00:00, 56.6MB/s]\n100%|██████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 1.57MB/s]\n100%|██████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 14.5MB/s]\n100%|██████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 11.3MB/s]\nEpoch 1 | Train Acc: 93.23%\nEpoch 2 | Train Acc: 98.46%\nEpoch 3 | Train Acc: 98.99%\nEpoch 4 | Train Acc: 99.30%\nEpoch 5 | Train Acc: 99.43%\n\nTest Accuracy: 99.11%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FSDP with 2 GPUs","metadata":{}},{"cell_type":"code","source":"%%writefile train_fsdp.py\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom functools import partial\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision import datasets, transforms\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    ShardingStrategy,\n)\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n\n\n# ------------------ FSDP SETUP ------------------\ndef fsdp_setup():\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\n\n    dist.init_process_group(\n        backend=\"nccl\",\n        device_id=local_rank\n    )\n    return local_rank\n\n\n# ------------------ MODEL ------------------\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n        self.conv3 = nn.Conv2d(64, 128, 5, padding=2)\n        self.pool = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(128 * 3 * 3, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = self.pool(self.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# ------------------ MAIN ------------------\ndef main():\n    local_rank = fsdp_setup()\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n\n    if rank == 0:\n        print(f\"Running FSDP on {world_size} GPUs\")\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    # Download dataset only once\n    if rank == 0:\n        datasets.MNIST(\"./data\", train=True, download=True)\n        datasets.MNIST(\"./data\", train=False, download=True)\n    dist.barrier()\n\n    train_dataset = datasets.MNIST(\"./data\", train=True, transform=transform)\n    train_sampler = DistributedSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=128,\n        sampler=train_sampler,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n    test_loader = DataLoader(test_dataset, batch_size=128)\n\n    model = CNN().cuda()\n\n    # Auto-wrap policy (wrap layers larger than threshold)\n    auto_wrap_policy = partial(\n    size_based_auto_wrap_policy,\n    min_num_params=1_000_000\n)\n\n    model = FSDP(\n        model,\n        auto_wrap_policy=auto_wrap_policy,\n        sharding_strategy=ShardingStrategy.FULL_SHARD,\n        device_id=local_rank\n    )\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    # ------------------ TRAIN ------------------\n    for epoch in range(5):\n        model.train()\n        train_sampler.set_epoch(epoch)\n\n        correct = total = 0\n        for x, y in train_loader:\n            x = x.cuda(non_blocking=True)\n            y = y.cuda(non_blocking=True)\n\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n\n            correct += (out.argmax(1) == y).sum().item()\n            total += y.size(0)\n\n        if rank == 0:\n            print(f\"Epoch {epoch+1} | Train Acc: {100*correct/total:.2f}%\")\n\n# ------------------ EVAL (ALL RANKS PARTICIPATE) ------------------\n    model.eval()\n    correct = torch.tensor(0, device=\"cuda\")\n    total = torch.tensor(0, device=\"cuda\")\n    \n    with torch.no_grad():\n        for x, y in test_loader:\n            x = x.cuda(non_blocking=True)\n            y = y.cuda(non_blocking=True)\n    \n            out = model(x)\n            correct += (out.argmax(1) == y).sum()\n            total += y.numel()\n    \n    # Reduce results to rank 0\n    dist.reduce(correct, dst=0, op=dist.ReduceOp.SUM)\n    dist.reduce(total, dst=0, op=dist.ReduceOp.SUM)\n    \n    if rank == 0:\n        print(f\"\\nTest Accuracy: {100 * correct.item() / total.item():.2f}%\")\n\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:25:15.801909Z","iopub.execute_input":"2026-01-05T07:25:15.802296Z","iopub.status.idle":"2026-01-05T07:25:15.810353Z","shell.execute_reply.started":"2026-01-05T07:25:15.802263Z","shell.execute_reply":"2026-01-05T07:25:15.809607Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_fsdp.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!torchrun --standalone --nproc_per_node=2 train_fsdp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:25:16.107423Z","iopub.execute_input":"2026-01-05T07:25:16.107758Z","iopub.status.idle":"2026-01-05T07:26:05.519317Z","shell.execute_reply.started":"2026-01-05T07:25:16.107731Z","shell.execute_reply":"2026-01-05T07:26:05.518130Z"}},"outputs":[{"name":"stdout","text":"W0105 07:25:18.235000 819 torch/distributed/run.py:774] \nW0105 07:25:18.235000 819 torch/distributed/run.py:774] *****************************************\nW0105 07:25:18.235000 819 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0105 07:25:18.235000 819 torch/distributed/run.py:774] *****************************************\n[W105 07:25:18.598438027 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:25:18.599278289 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:25:22.198142807 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:25:22.198297641 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:25:22.199048230 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W105 07:25:22.199598018 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nRunning FSDP on 2 GPUs\nEpoch 1 | Train Acc: 92.55%\nEpoch 2 | Train Acc: 98.55%\nEpoch 3 | Train Acc: 98.96%\nEpoch 4 | Train Acc: 99.26%\nEpoch 5 | Train Acc: 99.45%\n\nTest Accuracy: 99.12%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}